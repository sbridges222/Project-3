<html>
<head>
<title>AI, Algorithms, & Bias</title>
<link rel="stylesheet" href="stylepage.css">
</head>
<body>
<h1>AI, Algorithms, & Bias</h1>
<div class="menu center">
  <ul>
    <li><a href="index.html">Test my algorithm</a></li>
  </ul>
</div>
<h3>Watch my algorithim at work:</h3>
<iframe src="https://uwprod-my.sharepoint.com/personal/sbridges2_wisc_edu/_layouts/15/embed.aspx?UniqueId=1b7aa0ef-ca90-4a08-89a6-10a2c419fd48&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="640" height="360" frameborder="0" scrolling="no" allowfullscreen title="Video Project 3.mp4"></iframe>
<br></br>
</body>
<body>
<h3>What is algorithmic bias?</h3>
Algorithmic bias in machine learning models stems from the lack of diversity in training data and the failure to address existing biases within datasets. This can cause the algorithms to perpetuate societal biases and yield discriminatory results, especially concerning when applied to critical domains like hiring, lending, criminal justice, and healthcare. 
“Unmasking AI” by Joy Buolamwini is focused on the disparity in AI facial recognition technology’s accuracy across racial groups, and how the underrepresentation of diverse racial groups in datasets is connected to lack of diversity of the developer population in Silicon Valley. Facial recognition algorithms have been predominantly trained on the faces of white individuals, and as a result, do not consistently recognize the faces of other races and ethnicities. 
<h3>What causes algorithmic bias?</h3>
It is no coincidence that the population of developers who select the datasets are predominantly white, which can be attributed to egocentrism and affinity bias, which further exacerbate racial bias. Egocentrism, a common human trait, leads individuals to perceive their own experiences and worldviews as universal. As a result, this phenomenon can limit our understanding of diverse perspectives. Affinity bias, on the other hand, causes people to gravitate toward others who are like themselves. This reinforces homogeneity and further limits exposure to diverse perspectives. As a result, developers who are in a predominantly white work environment, may have also grown up in predominantly white cities or schools. Though the white population on a global scale is a minority in terms of number, to individuals in predominantly white environments, that may not seem to be the case. This phenomenon can lead developers to overrepresent their own identities, such as race or ethnicity, in the datasets used to train machine learning models, thereby creating algorithmic bias.
<h3>Application - Creating algorithms with bias in mind</h3>
<p>Reflecting on these challenges, it is critical to consider not only the data selected for training machine learning models but also the reasons behind those selections. In the case of developing a facial recognition model, for example, it becomes essential to include racially diverse samples to ensure accuracy across different demographic groups. This awareness of the impact of data selection extends to the testing phase as well, where the goal is to create models that are replicable and applicable to a diverse range of users.</p>
<p>In my personal experience with training machine learning models, such considerations became evident. I initially trained a model to identify if the image was of my own face with and without another object. After further reflection on the purpose of the exercise, however, it was apparent that the model needed to be able to be applied more broadly by my instructor and peers testing the model. I trained the model on various poses, but then realized that I would not be able to incorporate individuals of different racial/ethnic backgrounds in the dataset.  Based on Buolamwini’s work, I knew that training the model on images of a white individual would likely result in the model not working for others. Recognizing the importance of inclusivity, I decided to train the model on common household objects that others testing the model likely have access to. I trained the model on three classes of objects: plant, ball, and cup.</p>
<p>My process for collecting data samples to train the machine was also inspired by Buolamwini's findings, specifically in terms of creating diverse datasets for machine learning. I trained the machine on a minimum of three variations of each object to ensure that it would recognize the general type of object rather than the exact one(s) it was being trained on. I also varied the lighting, proximity to the camera, and angles of the objects when collecting data samples. I then tested the algorithm for bias using objects that were different from those the algorithm was trained on. The algorithm consistently and accurately recognized the object type in these tests.  emphasizing the significance of diverse and comprehensive datasets in ensuring the robustness and reliability of machine learning models across various conditions.</p>
<h3>Application - Learnings & recommendations</h3>
<p>If this algorithm were to be used on a broader scale, it would be important to include a comparable representation of variations in the dataset. For instance, a wide variety of plant types, or a wide variety of ball shapes and sizes, or a wide variety of drinkware (glasses, water bottles, thermos, etc.). It would also be important to continuously audit the results of the algorithm for potential biases or skews. These audits could involve testing the algorithm on individuals across a wide range of identities, tracking the performance of the algorithm by group, and expanding representation in datasets for groups experiencing the lowest algorithmic performance. These audits would generate statistics on the ratios of representation across identities that consistently provided ideal algorithmic performance, thereby informing industry best practices on dataset selection and representation.</p>
<p>It is important to remember that the image recognition is one of limitless applications of machine learning. Bias in image recognition can be minimized with diverse datasets, however other applications of machine learning are far more complex when it comes to evaluating datasets for bias. This can be attributed to lack of visibility to the reasoning behind the output of these systems in addition to the complexity of the inputs, outputs, and data sets. This reality underscores the criticality of efforts to identify, disclose, and mitigate algorithmic bias throughout the development process. This complex issue requires a multi-prong approach, involving increasing diversity developer populations, continuing education for developers on recognizing and minimizing bias, and rigorous testing and auditing protocols. Auditing protocols could include assessing inputs, outputs, proxies, and target variables for potential biases, as well as monitoring algorithm performance across different demographic groups. It is will also continue to become increasingly crucial to develop a governance structure for ongoing bias mitigation efforts, which ideally would include disclosure of potential biases identified in the algorithms to stakeholders as part of the development oversight process. By acknowledging and actively mitigating biases at every stage of model development, it becomes possible to create more equitable and inclusive AI systems that contribute positively to society.</p>
</body>
</html>
